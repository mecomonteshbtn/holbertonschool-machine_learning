# 0x01. Classification

## Read or watch:

*    [Supervised vs. Unsupervised Machine Learning](https://blogs.oracle.com/datascience/supervised-vs-unsupervised-machine-learning)
*    [How would you explain neural networks to someone who knows very little about AI or neurology?](https://www.quora.com/How-would-you-explain-neural-networks-to-someone-who-knows-very-little-about-AI-or-neurology/answer/Yohan-John)
*    [Using Neural Nets to Recognize Handwritten Digits](http://neuralnetworksanddeeplearning.com/chap1.html)
*    [Forward propagation](https://www.youtube.com/watch?v=wL17g67vU88)
*    [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
*    [Loss function](https://en.wikipedia.org/wiki/Loss_function)
*    [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
*    [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)
*    [Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)
*    [What is a Neural Network?](https://www.youtube.com/watch?v=n1l-9lIMW7E&index=2&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Supervised Learning with a Neural Network](https://www.youtube.com/watch?v=BYGpKPY9pO0&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&t=0s&index=4)
*    [Binary Classification](https://www.youtube.com/watch?v=eqEc66RFY0I&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=7)
*    [Logistic Regression](https://www.youtube.com/watch?v=hjrYrynGWGA&index=8&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Logistic Regression Cost Function](https://www.youtube.com/watch?v=SHEPb1JHw5o&index=9&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Gradient Descent](https://www.youtube.com/watch?v=uJryes5Vk1o&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=10)
*    [Computation Graph](https://www.youtube.com/watch?v=hCP1vGoCdYU&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=13)
*    [Logistic Regression Gradient Descent](https://www.youtube.com/watch?v=z_xiwjEdAC4&index=15&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Vectorization](https://www.youtube.com/watch?v=qsIrQi0fzbY&index=17&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Vectorizing Logistic Regression](https://www.youtube.com/watch?v=okpqeEUdEkY&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=19)
*    [Vectorizing Logistic Regressionâ€™s Gradient Computation](https://www.youtube.com/watch?v=2BkqApHKwn0&index=20&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [A Note on Python/Numpy Vectors](https://www.youtube.com/watch?v=V2QlTmh6P2Y&index=22&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Neural Network Representations](https://www.youtube.com/watch?v=CcRkHl75Z-Y&index=26&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Computing Neural Network Output](https://www.youtube.com/watch?v=rMOdrD61IoU&index=27&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Vectorizing Across Multiple Examples](https://www.youtube.com/watch?v=xy5MOQpx3aQ&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=28)
*    [Gradient Descent For Neural Networks](https://www.youtube.com/watch?v=7bLEWDZng_M&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=33)
*    [Random Initialization](https://www.youtube.com/watch?v=6by6Xas_Kho&index=35&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Deep L-Layer Neural Network](https://www.youtube.com/watch?v=2gw5tE2ziqA&index=36&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
*    [Train/Dev/Test Sets](https://www.youtube.com/watch?v=1waHlpKiNyY&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)
*    [Random Initialization For Neural Networks : A Thing Of The Past](https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e)
*    [Initialization of deep networks](http://deepdish.io/2015/02/24/network-initialization/)
*    [Multiclass classification](https://en.wikipedia.org/wiki/Multiclass_classification)
*    [Derivation: Derivatives for Common Neural Network Activation Functions](https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/)
*    [What is One Hot Encoding? Why And When do you have to use it?](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f?gi=a4f47cf027f7)
*    [Softmax function](https://en.wikipedia.org/wiki/Softmax_function)
*    [What is the intuition behind SoftMax function?](https://www.quora.com/What-is-the-intuition-behind-SoftMax-function)
*    [Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)
*    [Loss Functions: Cross-Entropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy)
*    [Softmax Regression](https://www.youtube.com/watch?v=LLux1SW--oM&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=31)
*    [Training Softmax Classifier](https://www.youtube.com/watch?v=ueO_Ph0Pyqk&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=32)
*    [numpy.zeros](https://numpy.org/doc/1.18/reference/generated/numpy.zeros.html)
*    [numpy.random.randn](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html)
*    [numpy.exp](https://numpy.org/doc/1.18/reference/generated/numpy.exp.html)
*    [numpy.log](https://numpy.org/doc/1.18/reference/generated/numpy.log.html)
*    [numpy.sqrt](https://numpy.org/doc/1.18/reference/generated/numpy.sqrt.html)
*    [numpy.where](https://numpy.org/doc/1.18/reference/generated/numpy.where.html)
*    [numpy.max](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.amax.html)
*    [numpy.sum](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.sum.html)
*    [numpy.argmax](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.argmax.html)
*    [What is Pickle in python?](https://yasoob.me/2013/08/02/what-is-pickle-in-python/)
*    [pickle](https://docs.python.org/3/library/pickle.html)
*    [pickle.dump](https://docs.python.org/3/library/pickle.html#pickle.dump)
*    [pickle.load](https://docs.python.org/3/library/pickle.html#pickle.load)

Optional:

*    [Predictive analytics](https://en.wikipedia.org/wiki/Predictive_analytics)
*    [Maximum Likelihood Estimation](https://towardsdatascience.com/maximum-likelihood-estimation-984af2dcfcac)

---
## General
*    What is a model?
*    What is supervised learning?
*    What is a prediction?
*    What is a node?
*    What is a weight?
*    What is a bias?
*    What are activation functions?
*        Sigmoid?
*        Tanh?
*        Relu?
*        Softmax?
*    What is a layer?
*    What is a hidden layer?
*    What is Logistic Regression?
*    What is a loss function?
*    What is a cost function?
*    What is forward propagation?
*    What is Gradient Descent?
*    What is back propagation?
*    What is a Computation Graph?
*    How to initialize weights/biases
*    The importance of vectorization
*    How to split up your data
*    What is multiclass classification?
*    What is a one-hot vector?
*    How to encode/decode one-hot vectors
*    What is the softmax function and when do you use it?
*    What is cross-entropy loss?
*    What is pickling in Python?

---
## General Requirements
*    Allowed editors: vi, vim, emacs
*    All your files will be interpreted/compiled on Ubuntu 16.04 LTS using python3 (version 3.5)
*    Your files will be executed with numpy (version 1.15)
*    All your files should end with a new line
*    The first line of all your files should be exactly #!/usr/bin/env python3
*    A README.md file, at the root of the folder of the project, is mandatory
*    Your code should use the pycodestyle style (version 2.4)
*    All your modules should have documentation (python3 -c 'print(__import__("my_module").__doc__)')
*    All your classes should have documentation (python3 -c 'print(__import__("my_module").MyClass.__doc__)')
*    All your functions (inside and outside a class) should have documentation (python3 -c 'print(__import__("my_module").my_function.__doc__)' and python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)')
*    Unless otherwise noted, you are not allowed to import any module except import numpy as np
*    Unless otherwise noted, you are not allowed to use any loops (for, while, etc.)
*    All your files must be executable
*    The length of your files will be tested using wc

---
## More Info

### Matrix Multiplications
For all matrix multiplications in the following tasks, please use [numpy.matmul](https://numpy.org/doc/1.18/reference/generated/numpy.matmul.html)

---
## Authors

* **Robinson Montes** - [mecomonteshbtn](https://github.com/mecomonteshbtn)
